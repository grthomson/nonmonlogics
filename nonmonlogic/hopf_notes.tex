\documentclass[a4paper,11pt]{report}
\usepackage{amsmath,amsfonts,amssymb,amsthm,hyperref, marvosym, accents}
%\usepackage{fullpage}
\usepackage{bm}
\usepackage{cleveref}
\usepackage{mdframed}
\usepackage{csquotes}
\usepackage[super]{natbib}
\usepackage{minitoc}
\usepackage{wasysym}
\usepackage{setspace}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{tikz-cd}
\usepackage{tikz}

\begin{document}
Logic first approach to both nonmonotonic and monotonic entailment

considerations: 


``There should be a way back:
conclusions reached in the simpler context, where the unessential is
forgotten, should be applicable to the original, more complicated,
context. Controlled forgetting, such as we have in abstraction, is
certainly a major characteristic of mathematical rationality, and an
embodiment of it is found in the concept of adjunction"

``The symbolic [13] and distributional [36] theories of meaning are somewhat orthogonal with competing pros and cons: the former is compositional but only qualitative, the latter is non-compositional but quantitative."

recall the "arbitrary substitution of expression" approach to logicality. this is captured by the lambda calculus of course

can I RECOVER a lambda calculus as the limiting behaviour of the nonmonotonic space?

``. You can define a notion of "Hopf object" in arbitrary symmetric monoidal categories, where you also need to specify the existence of a coproduct map. It's a matter of taste, but I think this general definition should actually be called "group object", so that Hopf algebras would be group objects in the category of vector spaces. The basic observation is that in a cartesian category, every object have a unique coalgebra structure given by the "diagonal" map so this part of the structure is forced in that case, so that those definitions are compatible."

Relevance can be seen as non-monotonicity of the inference connector of the system. Linear logic is relevant, in that the provability of $\vdash A \multimap B $
 does not imply the provability of $\vdash X \otimes A \multimap B $
. Relevance is a sort of inner non-monotonicity of the logic.

On the other side, what people call non-monotonic logics are systems where the provability itself of the system is not monotone: adding a new element to the set of formulas changes the set of provable formulas. It is a form of meta non-monotonicity, because it concerns provability and not the connector of inference. Linear logic is monotone: you can add whatever you want to the set of formulas, and any new axiom or inference rule to the system, but if you had a proof of the sequent $\Gamma \vdash M : A $
 before, you will still have it now, for you have not changed the other inference rules of the sequent calculus.

As far as I know, (real) non-monotonic logics are hard to put down in a sequent calculus form enjoying cut-elimination, or any other type of proof system with an equivalent notion of terminating proof-reduction. This is why the tradition categorical semantic approaches hardly would work for them.
\[
\frac{\Gamma \vdash \Delta}{\Gamma, A \vdash \Delta} \quad \text{(Weakening L)}
\]

% Weakening Right
\[
\frac{\Gamma \vdash \Delta}{\Gamma \vdash \Delta, A} \quad \text{(Weakening R)}
\]

% Cut Rule
\[
\frac{\Gamma \vdash \Delta, A \quad A, \Gamma' \vdash \Delta'}{\Gamma, \Gamma' \vdash \Delta, \Delta'} \quad \text{(Cut)}
\]


``This way, the forgetting of the forgetful functor is controlled.
Some conclusions we may reach by reasoning in B can be transferred
back to A." 

``Lambek seems to like monoidal categories where every object has a right and a left dual. One example is the category of representations of a non-cocommutative Hopf algebra. There are lots of these, and some have categories of representations that are pretty easy to compute with. So, you could try generalizing vector space models of meaning to models that use such categories."

deductive fragment is where the weakening rule holds, and should furthermore have some concept of "plugging in" or abstraction to give a lambda(-like) calculus. 

the goal here is to define the (deductive) logical fragment of the generally nonmonotonic inference system 

we would tag the fragment for which the antipode is invertible - this data may require 2-morphisms. cf. hopf 2-algebras and takeuchi, free hopf algebras generated by coalgebras

`` no left adjoint to the forgetful functor from Hopf algebras to vector spaces. However, Takeuchi in M. Takeuchi, Free Hopf algebras generated by coalgebras, J. Math. Soc. Japan 23 (1971), No.4, pp. 561–582.
constructed a left adjoint to the forgetful functor from Hopf algebras to coalgebras"

``1.2. Vector spaces over a field as a symmetric monoidal category
" - Baker notes

consider the move from the vector space (distributional meaning) to a coalgebra given by free \dots

Higher-Order DisCoCat : https://arxiv.org/pdf/2311.17813
this is arguably the most relevant paper -- note, i want not just logicality but mathematicological fragment 

see also: https://arxiv.org/pdf/1810.10297



Let \((C, \Delta, \varepsilon)\) be a \(k\)-coalgebra, and \((A, \mu, \eta)\) a \(k\)-algebra. Inside the \(k\)-vector space of linear maps from \(C\) to \(A\), i.e., inside \(\mathrm{Hom}_k(C, A)\), we define a bilinear multiplication between \(f, g \in \mathrm{Hom}_k(C, A)\) through:

\[
C \xrightarrow{\Delta} C \otimes C \xrightarrow{f \otimes g} A \otimes A \xrightarrow{\mu} A
\]

In other words, for any \(f, g \in \mathrm{Hom}_k(C, A)\), we define:
\[
f \star g = \mu \circ (f \otimes g) \circ \Delta
\]

Or equivalently, in Sweedler's notation:
\[
(f \star g)(c) = \sum f(c_{(1)}) g(c_{(2)})
\quad \text{for any } c \in C
\]

The multiplication \(\star\) defined above is clearly bilinear, and one can easily verify that \(f \star g \in \mathrm{Hom}_k(C, A)\). This multiplication \(\star\) is called the **convolution product**.

Now, it is relatively easy to show that the \(k\)-vector space \(\mathrm{Hom}_k(C, A)\), equipped with the convolution product, becomes a \(k\)-algebra. Associativity follows from a straightforward computation, while the unit element is \(\eta \circ \varepsilon \in \mathrm{Hom}_k(C, A)\).

\bigskip

Let us now pass to a concrete example of the above construction. Let \(H\) be a \(k\)-bialgebra. Denote by \(H_c\) the underlying \(k\)-coalgebra and by \(H_a\) the underlying \(k\)-algebra. Inside the \(k\)-vector space \(\mathrm{Hom}_k(H_c, H_a)\), i.e., inside \(\mathrm{End}_k(H)\), we consider the convolution product and the corresponding algebra structure defined above. Note that the identity map \(\mathrm{Id}_H: H \to H\) is an element of this algebra.

\textbf{Definition.} Let \(H\) be a \(k\)-bialgebra. A linear map \(S: H \to H\) is called an **antipode** of the \(k\)-bialgebra \(H\) if \(S\) is the inverse of the identity map \(\mathrm{Id}_H: H \to H\) with respect to the convolution product. In other words, if:
\[
S \star \mathrm{Id}_H = \mathrm{Id}_H \star S = \eta \circ \varepsilon
\]
or equivalently,
\[
\mu \circ (S \otimes \mathrm{Id}) \circ \Delta = \mu \circ (\mathrm{Id} \otimes S) \circ \Delta = \eta \circ \varepsilon
\]
or equivalently, using Sweedler's notation:
\[
\sum S(h_{(1)}) h_{(2)} = \sum h_{(1)} S(h_{(2)}) = \varepsilon(h) 1_H
\quad \text{for any } h \in H
\]

Note that according to the above definition, a given bialgebra does not necessarily possess an antipode. However, if it does, it is easy to show that the antipode is unique (the proof is analogous to the uniqueness of inverses in a group).

Intuitively, the antipode can be viewed as a generalization of the inverse element in a group. This interpretation is supported by the fact that for group Hopf algebras—which are among the first and most studied examples of Hopf algebras—the antipode map is given by:
\[
S(g) = g^{-1}
\]

The importance of the antipode, from the perspective of representation theory, lies in the fact that the presence of an antipode (i.e., the Hopf algebra structure) allows one to construct dual representations in a manner reminiscent of group representation theory. See, for instance, Chapter 9, Section 9.3, page 442 for more details.

IDEA: SHOULD WE BE ENCODING HERE STRUCTURAL RULES RATHER THAN THE INFERENCE RULES?


\end{document}